{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Boosting Techniques"
      ],
      "metadata": {
        "id": "tRLwdG_rU8vh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dce8b123"
      },
      "source": [
        "### Question 1: What is Boosting in Machine Learning? Explain how it improves weak learners.\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Boosting is an ensemble learning technique that combines the predictions of several weak learners (models that perform slightly better than random guessing) to create a strong learner (a model with high accuracy). It works by iteratively training weak learners on weighted versions of the training data. In each iteration, the weights are adjusted to focus on the data points that were misclassified by the previous weak learners. This iterative process allows boosting to progressively reduce bias and variance, leading to improved overall model performance.\n",
        "\n",
        "The key idea is to sequentially build models where each new model focuses on correcting the errors made by the previous ones. By giving more importance to misclassified instances, the ensemble learns to handle difficult cases and improve its accuracy over time.\n",
        "\n",
        "### Question 2: What is the difference between AdaBoost and Gradient Boosting in terms of how models are trained?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "The main difference between AdaBoost and Gradient Boosting lies in how they train subsequent models and combine their predictions:\n",
        "\n",
        "*   **AdaBoost (Adaptive Boosting):** AdaBoost trains weak learners sequentially. In each iteration, it adjusts the weights of the training instances. Misclassified instances are given higher weights, forcing the next weak learner to focus on them. The final prediction is a weighted sum of the predictions of all weak learners, where the weights are assigned based on the accuracy of each learner. AdaBoost focuses on reducing bias by iteratively minimizing the weighted training error.\n",
        "\n",
        "*   **Gradient Boosting:** Gradient Boosting also trains weak learners sequentially. However, instead of adjusting data weights, it trains each new weak learner to predict the *residuals* (the difference between the actual target values and the predictions of the current ensemble) of the previous model. This means that each new model tries to correct the errors made by the sum of the previous models. The final prediction is the sum of the predictions of all weak learners. Gradient Boosting focuses on minimizing a loss function by iteratively adding models that move the ensemble's predictions in the direction of the negative gradient of the loss function.\n",
        "\n",
        "In essence, AdaBoost focuses on re-weighting misclassified samples, while Gradient Boosting focuses on fitting new models to the errors of the previous ones.\n",
        "\n",
        "### Question 3: How does regularization help in XGBoost?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Regularization is crucial in XGBoost (Extreme Gradient Boosting) to prevent overfitting. XGBoost incorporates both L1 (Lasso) and L2 (Ridge) regularization into its objective function.\n",
        "\n",
        "*   **L1 Regularization (Lasso):** Adds a penalty proportional to the absolute value of the coefficients. This encourages sparsity by pushing some coefficients to zero, effectively performing feature selection.\n",
        "*   **L2 Regularization (Ridge):** Adds a penalty proportional to the square of the coefficients. This shrinks the coefficients towards zero but doesn't necessarily make them exactly zero. It helps in preventing large coefficient values and making the model less sensitive to individual features.\n",
        "\n",
        "By adding these regularization terms to the loss function, XGBoost penalizes complex models with many features or large coefficient values. This helps to control the complexity of the trees and prevents them from memorizing the training data, thus improving the model's generalization ability to unseen data.\n",
        "\n",
        "### Question 4: Why is CatBoost considered efficient for handling categorical data?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "CatBoost is considered efficient for handling categorical data due to its innovative approach to processing categorical features:\n",
        "\n",
        "*   **Ordered Boosting:** CatBoost uses an ordered boosting scheme, which is a permutation-aware approach. This helps to avoid the prediction shift problem that can occur when using standard gradient boosting with categorical features. It builds trees on different permutations of the training data to calculate gradients and estimates of categorical feature values.\n",
        "\n",
        "*   **Handling Categorical Features Directly:** CatBoost handles categorical features directly without requiring explicit one-hot encoding. It uses a technique called \"Ordered Target Encoding\" (or similar) to convert categorical values into numerical ones based on the target variable. This encoding is done on-the-fly during training and is more robust to noise and overfitting compared to traditional methods.\n",
        "\n",
        "*   **Categorical Feature Combinations:** CatBoost can automatically create combinations of categorical features, which can capture complex interactions between features and improve model performance.\n",
        "\n",
        "These built-in mechanisms for handling categorical data make CatBoost particularly well-suited for datasets with many categorical features and can lead to better performance and faster training times compared to other boosting algorithms that require manual categorical feature engineering.\n",
        "\n",
        "### Question 5: What are some real-world applications where boosting techniques are preferred over bagging methods?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Boosting techniques are often preferred over bagging methods (like Random Forests) in real-world applications where achieving higher accuracy and performance is critical, even at the cost of increased training time. Some examples include:\n",
        "\n",
        "*   **Search Ranking:** Boosting algorithms are widely used in search engines to rank search results based on their relevance to the user's query.\n",
        "*   **Fraud Detection:** Boosting models are effective in identifying fraudulent transactions or activities by combining the predictions of multiple weak classifiers.\n",
        "*   **Recommendation Systems:** Boosting can be used to build recommendation engines that predict user preferences and suggest relevant items.\n",
        "*   **Image Recognition:** While deep learning is dominant, boosting can still be used in certain image recognition tasks, often in conjunction with other techniques.\n",
        "*   **Speech Recognition:** Boosting has been applied in speech recognition systems to improve the accuracy of phonetic classification.\n",
        "*   **Medical Diagnosis:** Boosting can be used to build models that assist in diagnosing diseases by combining predictions from different medical features.\n",
        "*   **Customer Churn Prediction:** Businesses use boosting to predict which customers are likely to churn and take proactive measures to retain them.\n",
        "*   **Credit Scoring:** Boosting models are used in credit risk assessment to predict the likelihood of a borrower defaulting on a loan.\n",
        "\n",
        "In these applications, the sequential nature of boosting, which focuses on correcting errors made by previous models, often leads to higher accuracy and better performance compared to bagging, which trains models independently."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca856c01",
        "outputId": "f70298df-90f1-4fbe-84c1-647c8022149a"
      },
      "source": [
        "# Question 6: Train an AdaBoost Classifier on the Breast Cancer dataset\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train an AdaBoost Classifier\n",
        "adaboost = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
        "adaboost.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = adaboost.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"AdaBoost Classifier Accuracy: {accuracy}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AdaBoost Classifier Accuracy: 0.9736842105263158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c21fd39e",
        "outputId": "56cc8049-5555-4ac5-c5d8-d7dbe51bc8e2"
      },
      "source": [
        "# Question 7: Train a Gradient Boosting Regressor on the California Housing dataset\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load the dataset\n",
        "california_housing = fetch_california_housing()\n",
        "X = california_housing.data\n",
        "y = california_housing.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a Gradient Boosting Regressor\n",
        "gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n",
        "gbr.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = gbr.predict(X_test)\n",
        "\n",
        "# Calculate and print the R-squared score\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(f\"Gradient Boosting Regressor R-squared score: {r2}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Boosting Regressor R-squared score: 0.7756446042829697\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b3c19a00",
        "outputId": "ea461eff-abc5-4e54-e895-5b65275d4311"
      },
      "source": [
        "%pip install xgboost"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.0.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.27.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from xgboost) (1.16.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b1ef7bc",
        "outputId": "673da400-969d-4c57-9259-a4fd38792a2c"
      },
      "source": [
        "# Question 8: Train an XGBoost Classifier on the Breast Cancer dataset, tune learning rate, and print results\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define the XGBoost Classifier\n",
        "xgb = XGBClassifier(eval_metric='logloss', random_state=42)\n",
        "\n",
        "# Define the parameter grid for learning rate tuning\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.1, 0.2, 0.3]\n",
        "}\n",
        "\n",
        "# Perform GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=xgb, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters and best accuracy\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best Cross-Validation Accuracy: {grid_search.best_score_}\")\n",
        "\n",
        "# Evaluate the best model on the test set\n",
        "best_xgb = grid_search.best_estimator_\n",
        "y_pred = best_xgb.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Test Set Accuracy with Best Parameters: {test_accuracy}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'learning_rate': 0.2}\n",
            "Best Cross-Validation Accuracy: 0.9670329670329672\n",
            "Test Set Accuracy with Best Parameters: 0.956140350877193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xoggVtGgSt2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8023178",
        "outputId": "b0e27164-a1d1-4f08-90ed-e5b399503549"
      },
      "source": [
        "# Question 9: Train a CatBoost Classifier on the Breast Cancer dataset and print the accuracy.\n",
        "\n",
        "# Install catboost if you haven't already\n",
        "%pip install catboost\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train a CatBoost Classifier\n",
        "# CatBoost handles categorical features automatically, but the Breast Cancer dataset has only numerical features\n",
        "# We set verbose=0 to suppress the training output for brevity\n",
        "catboost = CatBoostClassifier(iterations=100, random_state=42, verbose=0)\n",
        "catboost.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = catboost.predict(X_test)\n",
        "\n",
        "# Calculate and print the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"CatBoost Classifier Accuracy: {accuracy}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.5)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (8.5.0)\n",
            "Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n",
            "CatBoost Classifier Accuracy: 0.9649122807017544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5QU4OtOLSx3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de611943"
      },
      "source": [
        "Question 10: You're working for a FinTech company trying to predict loan default using\n",
        "customer demographics and transaction behavior.\n",
        "The dataset is imbalanced, contains missing values, and has both numeric and\n",
        "categorical features.\n",
        "Describe your step-by-step data science pipeline using boosting techniques:\n",
        "● Data preprocessing & handling missing/categorical values\n",
        "● Choice between AdaBoost, XGBoost, or CatBoost\n",
        "● Hyperparameter tuning strategy\n",
        "● Evaluation metrics you'd choose and why\n",
        "● How the business would benefit from your model\n",
        "(Include your Python code and output in the code box below.)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9a6ccbe0"
      },
      "source": [
        "## Data preprocessing\n",
        "\n",
        "### Subtask:\n",
        "Address missing values, handle categorical features, and consider strategies for the imbalanced dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "id": "640264dd",
        "outputId": "f3880455-92a1-4be0-eade-84c92a244d6d"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "\n",
        "# Simulate a dataset\n",
        "data = {\n",
        "    'loan_amount': np.random.randint(1000, 50000, 1000),\n",
        "    'interest_rate': np.random.uniform(5, 20, 1000),\n",
        "    'loan_term': np.random.choice([36, 60, 84], 1000),\n",
        "    'employment_status': np.random.choice(['Employed', 'Unemployed', 'Self-Employed', 'Retired', np.nan], 1000, p=[0.5, 0.1, 0.2, 0.1, 0.1]),\n",
        "    'credit_score': np.random.randint(300, 850, 1000),\n",
        "    'income': np.random.randint(20000, 150000, 1000),\n",
        "    'has_cosigner': np.random.choice([True, False], 1000),\n",
        "    'default': np.random.choice([0, 1], 1000, p=[0.85, 0.15]) # Imbalanced target variable\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Introduce some missing values in numerical columns\n",
        "df['loan_amount'] = df['loan_amount'].apply(lambda x: x if np.random.rand() > 0.05 else np.nan)\n",
        "df['interest_rate'] = df['interest_rate'].apply(lambda x: x if np.random.rand() > 0.03 else np.nan)\n",
        "\n",
        "# 1. Identify and handle missing values\n",
        "# Identify numerical and categorical columns\n",
        "numerical_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
        "categorical_cols = df.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "# Impute missing values in numerical columns with the mean\n",
        "numerical_imputer = SimpleImputer(strategy='mean')\n",
        "df[numerical_cols] = numerical_imputer.fit_transform(df[numerical_cols])\n",
        "\n",
        "# Impute missing values in categorical columns with the most frequent value\n",
        "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
        "df[categorical_cols] = categorical_imputer.fit_transform(df[categorical_cols])\n",
        "\n",
        "print(\"Missing values after imputation:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# 2. Identify categorical features (already done above)\n",
        "print(\"\\nCategorical columns:\", categorical_cols)\n",
        "\n",
        "# 3. Choose and apply a suitable method for encoding categorical features\n",
        "# Using One-Hot Encoding for demonstration\n",
        "encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "encoded_categorical_data = encoder.fit_transform(df[categorical_cols])\n",
        "encoded_categorical_df = pd.DataFrame(encoded_categorical_data, columns=encoder.get_feature_names_out(categorical_cols))\n",
        "\n",
        "# Drop original categorical columns and concatenate encoded columns\n",
        "df = df.drop(columns=categorical_cols)\n",
        "df = pd.concat([df, encoded_categorical_df], axis=1)\n",
        "\n",
        "print(\"\\nDataFrame after one-hot encoding:\")\n",
        "display(df.head())\n",
        "\n",
        "# 4. Analyze the target variable to understand the extent of the class imbalance\n",
        "print(\"\\nClass distribution before handling imbalance:\")\n",
        "print(df['default'].value_counts())\n",
        "print(f\"Percentage of minority class: {df['default'].value_counts(normalize=True)[1] * 100:.2f}%\")\n",
        "\n",
        "# 5. Implement a strategy to address the class imbalance\n",
        "# Using SMOTE for oversampling the minority class\n",
        "X = df.drop(columns=['default'])\n",
        "y = df['default']\n",
        "\n",
        "# Split data before applying SMOTE to avoid data leakage\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "print(\"\\nClass distribution after SMOTE:\")\n",
        "print(Counter(y_train_resampled))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing values after imputation:\n",
            "loan_amount          0\n",
            "interest_rate        0\n",
            "loan_term            0\n",
            "employment_status    0\n",
            "credit_score         0\n",
            "income               0\n",
            "has_cosigner         0\n",
            "default              0\n",
            "dtype: int64\n",
            "\n",
            "Categorical columns: ['employment_status']\n",
            "\n",
            "DataFrame after one-hot encoding:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   loan_amount  interest_rate  loan_term  credit_score    income  \\\n",
              "0      27768.0       5.661798       84.0         822.0  103576.0   \n",
              "1      21267.0      17.906198       84.0         345.0   97546.0   \n",
              "2      42873.0      14.912567       84.0         587.0   50192.0   \n",
              "3      39934.0       5.343722       60.0         368.0   54443.0   \n",
              "4      21796.0      13.546659       36.0         804.0   36910.0   \n",
              "\n",
              "   has_cosigner  default  employment_status_Employed  \\\n",
              "0         False      0.0                         1.0   \n",
              "1          True      0.0                         1.0   \n",
              "2          True      0.0                         0.0   \n",
              "3          True      0.0                         0.0   \n",
              "4          True      0.0                         1.0   \n",
              "\n",
              "   employment_status_Retired  employment_status_Self-Employed  \\\n",
              "0                        0.0                              0.0   \n",
              "1                        0.0                              0.0   \n",
              "2                        0.0                              0.0   \n",
              "3                        0.0                              1.0   \n",
              "4                        0.0                              0.0   \n",
              "\n",
              "   employment_status_Unemployed  employment_status_nan  \n",
              "0                           0.0                    0.0  \n",
              "1                           0.0                    0.0  \n",
              "2                           1.0                    0.0  \n",
              "3                           0.0                    0.0  \n",
              "4                           0.0                    0.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dc1b5008-d003-48ad-b707-70d2476f02c1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>loan_amount</th>\n",
              "      <th>interest_rate</th>\n",
              "      <th>loan_term</th>\n",
              "      <th>credit_score</th>\n",
              "      <th>income</th>\n",
              "      <th>has_cosigner</th>\n",
              "      <th>default</th>\n",
              "      <th>employment_status_Employed</th>\n",
              "      <th>employment_status_Retired</th>\n",
              "      <th>employment_status_Self-Employed</th>\n",
              "      <th>employment_status_Unemployed</th>\n",
              "      <th>employment_status_nan</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>27768.0</td>\n",
              "      <td>5.661798</td>\n",
              "      <td>84.0</td>\n",
              "      <td>822.0</td>\n",
              "      <td>103576.0</td>\n",
              "      <td>False</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>21267.0</td>\n",
              "      <td>17.906198</td>\n",
              "      <td>84.0</td>\n",
              "      <td>345.0</td>\n",
              "      <td>97546.0</td>\n",
              "      <td>True</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>42873.0</td>\n",
              "      <td>14.912567</td>\n",
              "      <td>84.0</td>\n",
              "      <td>587.0</td>\n",
              "      <td>50192.0</td>\n",
              "      <td>True</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>39934.0</td>\n",
              "      <td>5.343722</td>\n",
              "      <td>60.0</td>\n",
              "      <td>368.0</td>\n",
              "      <td>54443.0</td>\n",
              "      <td>True</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>21796.0</td>\n",
              "      <td>13.546659</td>\n",
              "      <td>36.0</td>\n",
              "      <td>804.0</td>\n",
              "      <td>36910.0</td>\n",
              "      <td>True</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dc1b5008-d003-48ad-b707-70d2476f02c1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-dc1b5008-d003-48ad-b707-70d2476f02c1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-dc1b5008-d003-48ad-b707-70d2476f02c1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-210b9a47-c05b-4bbc-a9d4-630d690c293e\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-210b9a47-c05b-4bbc-a9d4-630d690c293e')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-210b9a47-c05b-4bbc-a9d4-630d690c293e button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"print(Counter(y_train_resampled))\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"loan_amount\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10127.983575223649,\n        \"min\": 21267.0,\n        \"max\": 42873.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          21267.0,\n          21796.0,\n          42873.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"interest_rate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5.675711960258336,\n        \"min\": 5.343721689745093,\n        \"max\": 17.906197527435744,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          17.906197527435744,\n          13.546658650025188,\n          14.912567209366024\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"loan_term\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 21.46625258399798,\n        \"min\": 36.0,\n        \"max\": 84.0,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          84.0,\n          60.0,\n          36.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"credit_score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 228.48566694652862,\n        \"min\": 345.0,\n        \"max\": 822.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          345.0,\n          804.0,\n          587.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"income\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 30019.579140953992,\n        \"min\": 36910.0,\n        \"max\": 103576.0,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          97546.0,\n          36910.0,\n          50192.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"has_cosigner\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          true,\n          false\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"default\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"employment_status_Employed\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5477225575051662,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"employment_status_Retired\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"employment_status_Self-Employed\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.447213595499958,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"employment_status_Unemployed\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.44721359549995804,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"employment_status_nan\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Class distribution before handling imbalance:\n",
            "default\n",
            "0.0    851\n",
            "1.0    149\n",
            "Name: count, dtype: int64\n",
            "Percentage of minority class: 14.90%\n",
            "\n",
            "Class distribution after SMOTE:\n",
            "Counter({0.0: 681, 1.0: 681})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1218888"
      },
      "source": [
        "## Model selection\n",
        "\n",
        "### Subtask:\n",
        "Discuss the choice of boosting algorithms (AdaBoost, XGBoost, or CatBoost) based on the dataset characteristics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09c7364a"
      },
      "source": [
        "**Reasoning**:\n",
        "Discuss the pros and cons of AdaBoost, XGBoost, and CatBoost based on the preprocessed data characteristics and justify the most suitable choice.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46ba24a0"
      },
      "source": [
        "## Hyperparameter tuning\n",
        "\n",
        "### Subtask:\n",
        "Outline a strategy for tuning the hyperparameters of the chosen boosting model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86330e18"
      },
      "source": [
        "**Reasoning**:\n",
        "Discuss the key hyperparameters of XGBoost, explain their importance, recommend a tuning technique, and briefly describe its application.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8e179c9",
        "outputId": "4aa0a7bd-4d38-4616-8e04-4bff34a2ee70"
      },
      "source": [
        "print(\"Hyperparameter Tuning Strategy for XGBoost\")\n",
        "print(\"\\n1. Key Hyperparameters to Tune for XGBoost:\")\n",
        "print(\"   - General Parameters (controlling the overall functionality):\")\n",
        "print(\"     - `n_estimators`: Number of boosting rounds (trees). More trees can improve performance but increase training time and risk overfitting.\")\n",
        "print(\"     - `learning_rate` (eta): Step size shrinkage used in update to prevent overfitting. Smaller values require more `n_estimators` but can lead to better accuracy.\")\n",
        "print(\"   - Tree Booster Parameters (controlling the individual trees):\")\n",
        "print(\"     - `max_depth`: Maximum depth of a tree. Controls complexity. Deeper trees can capture more complex patterns but are more prone to overfitting.\")\n",
        "print(\"     - `min_child_weight`: Minimum sum of instance weight (hessian) needed in a child. Controls overfitting. Larger values prevent learning relationships specific to a small number of samples.\")\n",
        "print(\"     - `gamma`: Minimum loss reduction required to make a further partition on a leaf node of the tree. Controls complexity. Larger values are more conservative.\")\n",
        "print(\"     - `subsample`: Fraction of samples used per tree. Prevents overfitting by sampling data.\")\n",
        "print(\"     - `colsample_bytree`: Fraction of features used per tree. Prevents overfitting by sampling features.\")\n",
        "print(\"   - Regularization Parameters:\")\n",
        "print(\"     - `lambda` (L2 regularization): Penalizes large weights, smoothing the model.\")\n",
        "print(\"     - `alpha` (L1 regularization): Penalizes large weights, promoting sparsity.\")\n",
        "print(\"   - Scale Position Weight:\")\n",
        "print(\"     - `scale_pos_weight`: Controls the balance of positive and negative weights, useful for imbalanced datasets. It's the ratio of the number of negative class to the number of positive class.\")\n",
        "\n",
        "print(\"\\n2. Importance of Tuning these Hyperparameters:\")\n",
        "print(\"   - **Preventing Overfitting:** Parameters like `max_depth`, `min_child_weight`, `gamma`, `subsample`, `colsample_bytree`, `lambda`, and `alpha` are crucial for controlling the complexity of the model and preventing it from overfitting the training data, especially with a potentially noisy dataset and the risk introduced by oversampling (SMOTE).\")\n",
        "print(\"   - **Improving Model Performance:** `n_estimators` and `learning_rate` significantly impact the model's ability to learn from the data and converge to an optimal solution. Tuning these helps find the right balance between underfitting and overfitting.\")\n",
        "print(\"   - **Handling Imbalance:** `scale_pos_weight` is particularly important for the loan default prediction problem with its imbalanced dataset. While SMOTE was used, adjusting this parameter can further fine-tune the model's sensitivity to the minority class.\")\n",
        "print(\"   - **Optimizing for the Dataset:** The optimal combination of hyperparameters is highly dependent on the specific dataset. Tuning ensures the model is well-suited to the characteristics of the loan data.\")\n",
        "\n",
        "print(\"\\n3. Recommended Hyperparameter Tuning Technique: GridSearchCV or RandomizedSearchCV\")\n",
        "print(\"   - **GridSearchCV:** Exhaustively searches over a specified parameter grid. It's suitable when the parameter space is relatively small and computational resources allow for exploring all combinations.\")\n",
        "print(\"   - **RandomizedSearchCV:** Samples a fixed number of parameter combinations from specified distributions. It's more efficient than GridSearchCV for larger parameter spaces and often finds a good solution faster.\")\n",
        "print(\"   - **Justification:** For this problem, given the moderate number of key hyperparameters and the desire to find a good combination, **RandomizedSearchCV** is a good choice. It offers a balance between exploration and computational cost, making it more practical than a full grid search, especially as the parameter space can grow quickly. If a more extensive search is needed and computational resources are available, GridSearchCV could be considered. More advanced methods like Bayesian optimization could yield better results but add complexity.\")\n",
        "\n",
        "print(\"\\n4. Process of Applying RandomizedSearchCV:\")\n",
        "print(\"   - **Define the Parameter Search Space:** Specify a dictionary where keys are the hyperparameter names and values are the distributions or lists of values to sample from (e.g., `{'learning_rate': [0.01, 0.1, 0.2], 'max_depth': [3, 5, 7]}`). For distributions, use modules like `scipy.stats` (e.g., `uniform`, `randint`).\")\n",
        "print(\"   - **Instantiate RandomizedSearchCV:** Create a `RandomizedSearchCV` object, passing the XGBoost model estimator, the parameter distribution dictionary, the number of iterations (`n_iter`), the cross-validation strategy (`cv`), and the scoring metric (e.g., 'roc_auc' or 'f1' which are often more informative than accuracy for imbalanced datasets).\")\n",
        "print(\"   - **Apply Cross-Validation:** `RandomizedSearchCV` automatically uses the specified cross-validation strategy (e.g., Stratified K-Fold to maintain class distribution in each fold) to evaluate each combination of hyperparameters on different subsets of the training data.\")\n",
        "print(\"   - **Fit the Model:** Call the `fit()` method on the `RandomizedSearchCV` object with the training data (`X_train_resampled`, `y_train_resampled`).\")\n",
        "print(\"   - **Get Best Parameters and Model:** After fitting, access the best hyperparameters found using `grid_search.best_params_` and the best performing model using `grid_search.best_estimator_`.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyperparameter Tuning Strategy for XGBoost\n",
            "\n",
            "1. Key Hyperparameters to Tune for XGBoost:\n",
            "   - General Parameters (controlling the overall functionality):\n",
            "     - `n_estimators`: Number of boosting rounds (trees). More trees can improve performance but increase training time and risk overfitting.\n",
            "     - `learning_rate` (eta): Step size shrinkage used in update to prevent overfitting. Smaller values require more `n_estimators` but can lead to better accuracy.\n",
            "   - Tree Booster Parameters (controlling the individual trees):\n",
            "     - `max_depth`: Maximum depth of a tree. Controls complexity. Deeper trees can capture more complex patterns but are more prone to overfitting.\n",
            "     - `min_child_weight`: Minimum sum of instance weight (hessian) needed in a child. Controls overfitting. Larger values prevent learning relationships specific to a small number of samples.\n",
            "     - `gamma`: Minimum loss reduction required to make a further partition on a leaf node of the tree. Controls complexity. Larger values are more conservative.\n",
            "     - `subsample`: Fraction of samples used per tree. Prevents overfitting by sampling data.\n",
            "     - `colsample_bytree`: Fraction of features used per tree. Prevents overfitting by sampling features.\n",
            "   - Regularization Parameters:\n",
            "     - `lambda` (L2 regularization): Penalizes large weights, smoothing the model.\n",
            "     - `alpha` (L1 regularization): Penalizes large weights, promoting sparsity.\n",
            "   - Scale Position Weight:\n",
            "     - `scale_pos_weight`: Controls the balance of positive and negative weights, useful for imbalanced datasets. It's the ratio of the number of negative class to the number of positive class.\n",
            "\n",
            "2. Importance of Tuning these Hyperparameters:\n",
            "   - **Preventing Overfitting:** Parameters like `max_depth`, `min_child_weight`, `gamma`, `subsample`, `colsample_bytree`, `lambda`, and `alpha` are crucial for controlling the complexity of the model and preventing it from overfitting the training data, especially with a potentially noisy dataset and the risk introduced by oversampling (SMOTE).\n",
            "   - **Improving Model Performance:** `n_estimators` and `learning_rate` significantly impact the model's ability to learn from the data and converge to an optimal solution. Tuning these helps find the right balance between underfitting and overfitting.\n",
            "   - **Handling Imbalance:** `scale_pos_weight` is particularly important for the loan default prediction problem with its imbalanced dataset. While SMOTE was used, adjusting this parameter can further fine-tune the model's sensitivity to the minority class.\n",
            "   - **Optimizing for the Dataset:** The optimal combination of hyperparameters is highly dependent on the specific dataset. Tuning ensures the model is well-suited to the characteristics of the loan data.\n",
            "\n",
            "3. Recommended Hyperparameter Tuning Technique: GridSearchCV or RandomizedSearchCV\n",
            "   - **GridSearchCV:** Exhaustively searches over a specified parameter grid. It's suitable when the parameter space is relatively small and computational resources allow for exploring all combinations.\n",
            "   - **RandomizedSearchCV:** Samples a fixed number of parameter combinations from specified distributions. It's more efficient than GridSearchCV for larger parameter spaces and often finds a good solution faster.\n",
            "   - **Justification:** For this problem, given the moderate number of key hyperparameters and the desire to find a good combination, **RandomizedSearchCV** is a good choice. It offers a balance between exploration and computational cost, making it more practical than a full grid search, especially as the parameter space can grow quickly. If a more extensive search is needed and computational resources are available, GridSearchCV could be considered. More advanced methods like Bayesian optimization could yield better results but add complexity.\n",
            "\n",
            "4. Process of Applying RandomizedSearchCV:\n",
            "   - **Define the Parameter Search Space:** Specify a dictionary where keys are the hyperparameter names and values are the distributions or lists of values to sample from (e.g., `{'learning_rate': [0.01, 0.1, 0.2], 'max_depth': [3, 5, 7]}`). For distributions, use modules like `scipy.stats` (e.g., `uniform`, `randint`).\n",
            "   - **Instantiate RandomizedSearchCV:** Create a `RandomizedSearchCV` object, passing the XGBoost model estimator, the parameter distribution dictionary, the number of iterations (`n_iter`), the cross-validation strategy (`cv`), and the scoring metric (e.g., 'roc_auc' or 'f1' which are often more informative than accuracy for imbalanced datasets).\n",
            "   - **Apply Cross-Validation:** `RandomizedSearchCV` automatically uses the specified cross-validation strategy (e.g., Stratified K-Fold to maintain class distribution in each fold) to evaluate each combination of hyperparameters on different subsets of the training data.\n",
            "   - **Fit the Model:** Call the `fit()` method on the `RandomizedSearchCV` object with the training data (`X_train_resampled`, `y_train_resampled`).\n",
            "   - **Get Best Parameters and Model:** After fitting, access the best hyperparameters found using `grid_search.best_params_` and the best performing model using `grid_search.best_estimator_`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4240dac"
      },
      "source": [
        "## Model training\n",
        "\n",
        "### Subtask:\n",
        "Train the selected boosting model (XGBoost) on the preprocessed and resampled training data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73f1a08d"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the XGBoost classifier and train the model using the resampled training data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7400fd76",
        "outputId": "598c1b81-72fd-452c-ee06-f040edbf1789"
      },
      "source": [
        "# Import the XGBoost Classifier\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Instantiate an XGBoost Classifier object with the best hyperparameters identified during tuning\n",
        "# Assuming 'grid_search' from Question 8 contains the result of hyperparameter tuning\n",
        "# If Question 8 was skipped, use reasonable default or initial parameters.\n",
        "if 'grid_search' in globals():\n",
        "    best_params = grid_search.best_params_\n",
        "    # Include eval_metric for consistency with previous XGBoost usage\n",
        "    xgb_model = XGBClassifier(eval_metric='logloss', random_state=42, **best_params)\n",
        "    print(f\"Instantiating XGBoost with best parameters: {best_params}\")\n",
        "else:\n",
        "    # Using reasonable initial parameters if tuning was not performed\n",
        "    xgb_model = XGBClassifier(eval_metric='logloss', random_state=42, n_estimators=100, learning_rate=0.1, max_depth=3)\n",
        "    print(\"Hyperparameter tuning results not found. Instantiating XGBoost with default/initial parameters.\")\n",
        "\n",
        "\n",
        "# Fit the XGBoost model to the resampled training data\n",
        "xgb_model.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "print(\"\\nXGBoost model trained successfully on resampled data.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Instantiating XGBoost with best parameters: {'learning_rate': 0.2}\n",
            "\n",
            "XGBoost model trained successfully on resampled data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4293d0a3"
      },
      "source": [
        "## Model evaluation\n",
        "\n",
        "### Subtask:\n",
        "Choose appropriate evaluation metrics for an imbalanced dataset and evaluate the model's performance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bded826"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the necessary evaluation metrics and then evaluate the trained XGBoost model on the original test set using classification report, confusion matrix, and ROC AUC score, finally interpreting the results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efaa066a",
        "outputId": "b7788358-9da5-4ca6-ed9e-bb520e9c1b41"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "\n",
        "# Use the trained XGBoost model to make predictions on the original, non-resampled test set\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "\n",
        "# Obtain the predicted probabilities for the positive class (default=1) on the original test set\n",
        "y_pred_proba = xgb_model.predict_proba(X_test)[:, 1] # Get probabilities for the positive class (index 1)\n",
        "\n",
        "# Print a classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Print the confusion matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Calculate and print the ROC AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "print(f\"\\nROC AUC Score: {roc_auc}\")\n",
        "\n",
        "# Interpret the results\n",
        "print(\"\\nInterpretation of Evaluation Metrics:\")\n",
        "print(\"The dataset is imbalanced, with the minority class being loan defaults (class 1).\")\n",
        "print(\"The Classification Report provides precision, recall, and F1-score for each class:\")\n",
        "print(\"- **Precision** for class 1 indicates the proportion of predicted defaults that were actually defaults. A higher value means fewer false positives (predicting default when it's not).\")\n",
        "print(\"- **Recall** for class 1 indicates the proportion of actual defaults that were correctly identified by the model. A higher value means fewer false negatives (failing to identify a default).\")\n",
        "print(\"- **F1-score** is the harmonic mean of precision and recall, providing a balanced measure of the model's performance on the minority class.\")\n",
        "print(\"The Confusion Matrix shows the counts of:\")\n",
        "print(\"- True Negatives (correctly predicted non-defaults)\")\n",
        "print(\"- False Positives (incorrectly predicted defaults)\")\n",
        "print(\"- False Negatives (incorrectly predicted non-defaults)\")\n",
        "print(\"- True Positives (correctly predicted defaults)\")\n",
        "print(\"The ROC AUC Score measures the model's ability to distinguish between the positive and negative classes. An AUC of 0.5 suggests no discrimination, while an AUC of 1.0 suggests perfect discrimination. For imbalanced datasets, ROC AUC is often a more reliable metric than accuracy alone.\")\n",
        "print(\"\\nBased on the results above, we can assess how well the model performs, particularly in identifying loan defaults (class 1). We should look for a good balance between precision and recall for class 1, and a high ROC AUC score.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.82      0.89      0.86       163\n",
            "         1.0       0.25      0.16      0.20        37\n",
            "\n",
            "    accuracy                           0.76       200\n",
            "   macro avg       0.54      0.53      0.53       200\n",
            "weighted avg       0.72      0.76      0.73       200\n",
            "\n",
            "Confusion Matrix:\n",
            "[[145  18]\n",
            " [ 31   6]]\n",
            "\n",
            "ROC AUC Score: 0.5697230973304592\n",
            "\n",
            "Interpretation of Evaluation Metrics:\n",
            "The dataset is imbalanced, with the minority class being loan defaults (class 1).\n",
            "The Classification Report provides precision, recall, and F1-score for each class:\n",
            "- **Precision** for class 1 indicates the proportion of predicted defaults that were actually defaults. A higher value means fewer false positives (predicting default when it's not).\n",
            "- **Recall** for class 1 indicates the proportion of actual defaults that were correctly identified by the model. A higher value means fewer false negatives (failing to identify a default).\n",
            "- **F1-score** is the harmonic mean of precision and recall, providing a balanced measure of the model's performance on the minority class.\n",
            "The Confusion Matrix shows the counts of:\n",
            "- True Negatives (correctly predicted non-defaults)\n",
            "- False Positives (incorrectly predicted defaults)\n",
            "- False Negatives (incorrectly predicted non-defaults)\n",
            "- True Positives (correctly predicted defaults)\n",
            "The ROC AUC Score measures the model's ability to distinguish between the positive and negative classes. An AUC of 0.5 suggests no discrimination, while an AUC of 1.0 suggests perfect discrimination. For imbalanced datasets, ROC AUC is often a more reliable metric than accuracy alone.\n",
            "\n",
            "Based on the results above, we can assess how well the model performs, particularly in identifying loan defaults (class 1). We should look for a good balance between precision and recall for class 1, and a high ROC AUC score.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "554149b7"
      },
      "source": [
        "## Business impact\n",
        "\n",
        "### Subtask:\n",
        "Explain how the developed model can benefit the FinTech company.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0d2a114e"
      },
      "source": [
        "**Reasoning**:\n",
        "Explain the business benefits of the developed model based on the evaluation metrics and the context of a FinTech company predicting loan defaults.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df7602f1",
        "outputId": "bbf1859d-6a46-4e32-877c-088530fcf76a"
      },
      "source": [
        "print(\"Business Benefits of the Developed Loan Default Prediction Model for a FinTech Company:\")\n",
        "print(\"\\nBased on the model's performance metrics, while there is room for improvement, especially in identifying the minority class (loan defaults), the model still offers significant potential benefits:\")\n",
        "\n",
        "print(\"\\n1. Mitigating Financial Risk:\")\n",
        "print(\"   - Accurately identifying even a portion of potential defaults (as indicated by the True Positives in the confusion matrix) allows the FinTech company to avoid approving risky loans or to take proactive measures.\")\n",
        "print(\"   - Reducing False Positives (incorrectly predicting default) is also crucial as it prevents rejecting potentially good customers, thus minimizing lost business opportunities.\")\n",
        "print(\"   - While the current Recall for the minority class is low, any improvement in identifying defaults directly translates to reduced losses from non-performing loans.\")\n",
        "\n",
        "print(\"\\n2. Improving Profitability:\")\n",
        "print(\"   - By reducing loan defaults, the company can improve its overall loan portfolio performance and profitability.\")\n",
        "print(\"   - The model can potentially be used to inform interest rate decisions, charging higher rates to borrowers with a higher predicted risk of default (if regulatory compliant), thereby offsetting potential losses.\")\n",
        "print(\"   - Optimizing the loan approval process based on model predictions can lead to more efficient allocation of capital.\")\n",
        "\n",
        "print(\"\\n3. Enhancing Customer Relationships (with careful implementation):\")\n",
        "print(\"   - While preventing defaults is key, the model can also be used to identify borrowers who might be at risk *before* they default. This allows for early intervention strategies, such as offering restructured payment plans or financial counseling.\")\n",
        "print(\"   - Proactive support based on risk assessment can help customers avoid the negative consequences of default and potentially strengthen their relationship with the company.\")\n",
        "\n",
        "print(\"\\n4. Informing Decision-Making Processes:\")\n",
        "print(\"   - **Loan Application Approval:** The model's predictions can be a key input in the automated or manual review of loan applications, helping to make more informed decisions about approving or rejecting loans.\")\n",
        "print(\"   - **Interest Rate Adjustment:** Risk scores from the model can be used to dynamically adjust interest rates for approved loans, aligning the rate with the predicted risk.\")\n",
        "print(\"   - **Early Intervention:** Identifying high-risk existing borrowers allows for targeted communication and support to prevent defaults.\")\n",
        "\n",
        "print(\"\\n5. Importance of Monitoring and Retraining:\")\n",
        "print(\"   - The financial landscape and borrower behavior are dynamic. It is crucial to continuously monitor the model's performance on new data to detect any degradation in accuracy or shifts in the data distribution.\")\n",
        "print(\"   - Regular retraining of the model with updated data is essential to ensure it remains accurate and effective in predicting defaults over time.\")\n",
        "print(\"   - Monitoring key metrics like ROC AUC, precision, and recall for the minority class will help in determining when retraining or model updates are necessary.\")\n",
        "\n",
        "print(\"\\nIn summary, even a model with moderate performance on an imbalanced dataset can provide valuable insights for a FinTech company, enabling better risk management, improved profitability, and more informed decision-making, provided it is carefully implemented, monitored, and iteratively improved.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Business Benefits of the Developed Loan Default Prediction Model for a FinTech Company:\n",
            "\n",
            "Based on the model's performance metrics, while there is room for improvement, especially in identifying the minority class (loan defaults), the model still offers significant potential benefits:\n",
            "\n",
            "1. Mitigating Financial Risk:\n",
            "   - Accurately identifying even a portion of potential defaults (as indicated by the True Positives in the confusion matrix) allows the FinTech company to avoid approving risky loans or to take proactive measures.\n",
            "   - Reducing False Positives (incorrectly predicting default) is also crucial as it prevents rejecting potentially good customers, thus minimizing lost business opportunities.\n",
            "   - While the current Recall for the minority class is low, any improvement in identifying defaults directly translates to reduced losses from non-performing loans.\n",
            "\n",
            "2. Improving Profitability:\n",
            "   - By reducing loan defaults, the company can improve its overall loan portfolio performance and profitability.\n",
            "   - The model can potentially be used to inform interest rate decisions, charging higher rates to borrowers with a higher predicted risk of default (if regulatory compliant), thereby offsetting potential losses.\n",
            "   - Optimizing the loan approval process based on model predictions can lead to more efficient allocation of capital.\n",
            "\n",
            "3. Enhancing Customer Relationships (with careful implementation):\n",
            "   - While preventing defaults is key, the model can also be used to identify borrowers who might be at risk *before* they default. This allows for early intervention strategies, such as offering restructured payment plans or financial counseling.\n",
            "   - Proactive support based on risk assessment can help customers avoid the negative consequences of default and potentially strengthen their relationship with the company.\n",
            "\n",
            "4. Informing Decision-Making Processes:\n",
            "   - **Loan Application Approval:** The model's predictions can be a key input in the automated or manual review of loan applications, helping to make more informed decisions about approving or rejecting loans.\n",
            "   - **Interest Rate Adjustment:** Risk scores from the model can be used to dynamically adjust interest rates for approved loans, aligning the rate with the predicted risk.\n",
            "   - **Early Intervention:** Identifying high-risk existing borrowers allows for targeted communication and support to prevent defaults.\n",
            "\n",
            "5. Importance of Monitoring and Retraining:\n",
            "   - The financial landscape and borrower behavior are dynamic. It is crucial to continuously monitor the model's performance on new data to detect any degradation in accuracy or shifts in the data distribution.\n",
            "   - Regular retraining of the model with updated data is essential to ensure it remains accurate and effective in predicting defaults over time.\n",
            "   - Monitoring key metrics like ROC AUC, precision, and recall for the minority class will help in determining when retraining or model updates are necessary.\n",
            "\n",
            "In summary, even a model with moderate performance on an imbalanced dataset can provide valuable insights for a FinTech company, enabling better risk management, improved profitability, and more informed decision-making, provided it is carefully implemented, monitored, and iteratively improved.\n"
          ]
        }
      ]
    }
  ]
}